
## Context

> AI: non-Human mind/intelligence/blob in my context

-----

## One-liner

- Superintelligence is a matter of national security.
- People are generally not careful with their words. They rely on some vague notion of the words they learned through osmosis. This sloppy thinking extends beyond words and poisons thinking about concepts like AI. It’s one of the most frustrating things about discussing these topics with people. You have to manually break down their understanding of the words into something actually meaningful.
- Challenges faced by computers and human minds alike: how to manage finite space, finite time, limited attention, unknown unknowns, incomplete information, and an unforeseeable future; how to do so with grace and confidence; and how to do so in a community with others who are all simultaneously trying to do the same.
- 人工智能的先驱马文·明斯基这样说道：“尽管近代科学出现了一些思想萌芽，使得‘believe’（相信）、‘know’（知道）、‘mean’（意味着）这样的词语在日常生活中变得很常用，但严格来说，它们的定义似乎太过粗糙，以至于无法支撑强有力的理论……就如同目前的‘self’（自我）或‘understand’（理解）这样的词语对我们而言一样，它们尚处于通往更完善的概念的起步阶段。”明斯基继续指出：“我们对这些概念的混淆，源于传统思想不足以解决这一极度困难的问题……我们现在还处在关于心智的一系列概念的形成期。”
- 我们人类倾向于高估人工智能的发展速度，而低估人类自身智能的复杂性。
- 目前的人工智能与通用智能还相距甚远，并且我不认为超级智能已经近在眼前了。如果通用人工智能真的会实现，我敢保证，它的复杂性能够与我们人类的大脑相媲美。
- 人们担心计算机会变得过于聪明并接管世界，但真正的问题是计算机太愚蠢了，并且它们已经接管了世界。

## Multi-liner

### AI Underneath

> At a low level, there is no magic in machine learning — just lots of straightforward mathematics and systems engineering. And somehow the end result is entirely magical.

### What the Current Situation of Alignment is

> People often associate alignment with some complicated questions about human values, or jump to political controversies, but deciding on what behaviors and values to instill in the model, while important, is a separate problem. The primary problem is that for whatever you want to instill the model (including ensuring very basic things, like “follow the law”!) we don’t yet know how to do that for the very powerful AI systems we are building very soon.

### What's LLM in the landscape of AI field?

<img src="/_assets/infographic-ai-tech-relation.png" alt="AI n LLM" width="400" height=50% />

### On LLM (Large Language Model)

> Language models don't actually understand the world, they understand relationships between lexemes, they aren't intelligent in the same way that a motion picture isn't really moving.

### 什么算是 AGI (Artificial General Intelligence)

> 我觉得 Intelligence 最重要的能力，是根据认知（cognition）或知识（knowledge），从已见过的内容来辨认未见过的内容。比如我看到一只虫，我不知道这是什么虫，但从翅膀、触角、肢节推测，大概是某种甲虫。ChatGPT 已经可以做到这个了。我不期待 AI 也会有恶心、害怕、拿起拍子来打死虫子这类行为。

### We Need A Capable Successor to RLHF

> Even now, AI labs already [need to pay expert software engineers](https://www.nytimes.com/2024/04/10/technology/ai-chatbot-training-chatgpt.html) to give RLHF ratings for ChatGPT code—the code current models can generate is already pretty advanced! Human labeler-pay has gone from a few dollars for MTurk labelers to ~$100/hour for GPQA questions
>
> in the last few years. In the (near) future, even the best human experts spending lots of time won’t be good enough. We’re starting to hit early versions of the superalignment problem in the real world now, and very soon this will be a major issue even just for practically deploying next-generation systems.

### 使用 LLM 因材施教

> 如果一个学生可以用 GPT 个性化为自己写教材，这个教材使用的语言格式，分段模式都是按照她的偏好设置。所有的类比和概念描述都按照她的知识构成来编写。所有的练习题充分符合她的能力循序渐进，所有的 Lab 都是根据她目前手上的硬件设施为她量身定做。练习题也是为了提高她的能力设计而不是简单的测评筛选。这种教育不香吗？

### Reality Computing

> `= Spatial Computing + AI + IoT + 多模态交互`

- 👇感知 (perception) 建模 (model) 理解 (relation)
- 👆收集 (sensor) 汇整 (process) 计算 (compute)
